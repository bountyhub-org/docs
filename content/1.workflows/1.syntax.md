# Workflows

Workflows are the way you describe automation. They should describe the way you do stuff in a concise and declarative way. Workflows should act as if there is a replica of you doing the stuff that can be automated. It should be a continuous process, feeding you information in the real time, allowing you to do the best work you possibly can.

Pipelines on the other hand are sequences of steps that are executed from start to finish. It is much easier to reason about the pipeline, but it is more natural to work in terms of workflows.

## Workflows vs Pipelines

The main difference between workflows and pipelines is that pipelines are executed from start to finish. You can have parallelism in pipelines, but essentially, you are passing through stages. Let's take a simple example to understand the difference.

#### Pipeline example:

![pipeline](/pipeline.svg)

The main problem here is that you either execute the entire pipeline, or you don't. But that is not the way you would approach a target. You would use common data to execute multiple pipelines, and this is where workflows kick in.

Let's say I start with subdomain discovery. You have multiple tools that run subdomain discovery to maximize the target scope. However, from there, you can have multiple pipelines. For example:

1. Screenshot all subdomain landing pages to quickly pick a target
2. Run httprobe to see what subdomains are live
3. Run port scan on each target.
4. ...

It is not that often that ports are open, and scan is noisy, so we can run it every other day or once a week. However, running httprobe is relatively quick, and does not raise alarms that often, so we can do it every 6 hours.

This is an example of how a single stage may have multiple pipelines. And you are already doing that right now, I would guess.

## Terminology

| term | description |
| ---- | ----------- |
| scan | Scan is a description for the job. Scan describes what you want to run. |
| job  | Job is a single invocation of the scan. Each scan can have 0 or more jobs, while each job belongs to a single scan. |
| workflow | Workflow is a collection of scans. It is described in your workflow file. It is a unit, meaning the one workflow cannot use another workflow |
| pipeline | Collection of scans that depend on each other. It is a subset of the workflow. |

## Writing workflows

Workflows are yaml definitions where you describe what you want to do. YAML proved to be a great tool to describe your intent. It is a commonly used language to describe workflows.

Initially, writing workflows started with the UI editor, but proved to be much more complicated than the declarative way.

::alert{type="warning"}
Right now, the experience of writing workflows is horrible. Hopefully, language client will be implemented soon, but please, make sure to have this reference open if parsing fails...
::

The editor however does contain a basic syntax, so you can leverage autocompletes to see what fields are available.

## Workflow components

### `scans`

Each workflow is made of one or more `scans`. Each scan has a unique name, which is a key in the scans object. The name will be presented as `[ID]` for the rest of the document, since it is unique for each scan, and therefore, serves as an identifier.

You can have the unlimited number of scans, as long as their names are unique.

#### Example:

```yaml
scans:
  assetfinder:
    on:
      cron: 0 0 * * * *
    steps:
      - run: assetfinder example.com
        shell: bash
```

### `scans.[ID].on`

Serves as a trigger for a scan execution. This object contains specification that will trigger a scan on some event.

To start the pipeline, there must exist some event to trigger it. Events are raised automatically (on `cron`), or manually (on `dispatch`). From there, you build the next stage by running on `expr`.

### `scans.[ID].on.cron`

Cron defines a schedule based on which the workflows are executed.

Cron is described in form of:

| second   | minute   | hour     | day of month | month    | day of week |
| -------- | -------- | -------- | ------------ | -------- | ----------- |
| required | required | required | required     | required | required    |

If you need help specifying or testing your cron schedule, you can use [crontab guru](https://crontab.guru/). This is a great site that will help you write and understand the spec you write for the scheduler.

As a limitation, cron parser does not allow non-standard modifiers, such as `@hourly` etc. Currently supported specifications include:

- Wildcard (`*`)
- Lists (`,`)
- Ranges (`-`)
- Steps (`/`)

#### Example:

```yaml
scans:
  assetfinder:
    on:
      cron: 0 * * * * *
```

### `scans.[ID].on.expr`

This field allows you to specify expression that describes when should the scan be scheduled. Read more about expressions [here](/workflows/expressions)

Expressions are to describe the scan that should run after some other scan has finished. As an example, after I finish my subdomain discovery phase, I would love to trigger the `httprobe` to see what subdomains are live. To provide a simple example, let's say my subdomain discovery uses `assetfinder` and `subfinder`. Since they are starting points of my workflow, they must run either on cron, or on dispatch.

#### Example

```yaml
scans:
  assetfinder:
    on:
      cron: 0 1 0/12 * * *
    uploads:
      - output.txt
    steps:
    - run: assetfinder ${{ vars.DOMAIN }} > output.txt
      shell: bash
    - run: sort -o output.txt -u output.txt
      shell: bash

  subfinder:
    on:
      cron: 0 15 0/12 * * *
    uploads:
      - output.txt
    steps:
    - run: subfinder -d ${{ vars.DOMAIN }} > output.txt
      shell: bash
    - run: sort -o output.txt -u output.txt
      shell: bash

  probe:
    on:
      expr: |
        size(scans.assetfinder) > 0
        && size(scans.subfinder) > 0
        && scans.assetfinder[0].status == 'succeeded'
        && scans.subfinder[0].status == 'succeeded'
    uploads:
      - output.txt
    steps:
      - run: |
          mkdir assetfinder
          cd assetfinder
          bh job download -p ${{ project.id }} -w ${{ workflow.id }} -r ${{ revision.id }} -j ${{ scans.assetfinder[0].id }} -o output.zip
          unzip output.zip
          rm output.zip
        shell: bash
      - run: |
          mkdir subfinder
          cd subfinder
          bh job download -p ${{ project.id }} -w ${{ workflow.id }} -r ${{ revision.id }} -j ${{ scans.subfinder[0].id }} -o output.zip
          unzip output.zip
          rm output.zip
        shell: bash
      - run: |
          cat assetfinder/output.txt > subdomains.txt
          cat subfinder/output.txt >> subdomains.txt
          sort -o subdomains.txt -u subdomains.txt
        shell: bash
      # Run httprobe on unique subdomains
      - run: cat subdomains.txt | httprobe > output.txt
        shell: bash
      - run: sort -o output.txt -u output.txt
        shell: bash
```

### `scans.[ID].on.dispatch`

This field allows you to specify a scan that can start a pipeline. The idea behind this scan type is that you may have some expensive tools that you want to run on-demand. They should not be executed periodically, but they can start a pipeline of other tools.

#### Example

```yaml
scans:
  expensive:
    on:
      dispatch: {} # Important to add '{}', since dispatch is empty object in yaml
    steps:
      - run: echo "Executing expensive tool with API rate limit etc."
        shell: bash
  
  expensive-stage-2:
    on:
      expr: size(scans.expensive) > 0 && scans.expensive[0].status == 'succeeded'
    steps:
      - run: echo "You continue normally from here"
        shell: bash
```

### `scans.[ID].uploads[]`

Uploads field specifies what files or directories are uploaded to the server when the scan is *successful*. If the scan fails, no files will be uploaded to the server.

Uploads are required if you have any dependencies between scans. They will not show up in the UI, but you have a button to download these results. In the UI, only stdout and stderr is displayed. The reader for stdout is limited at 2MiB so please, do not try to print everything to the stdout or stderr.

#### Example

```yaml
scans:
  example:
    on: 
      cron: 0 0 0 * * *
    uploads:
      - file.txt
      - outdir/subdir
    steps:
      - run: echo "Contents of the file" > file.txt
        shell: bash
      - run: mkdir -p outdir/subdir
        shell: bash
      - run: |
          echo "Contents of file 1 in subdir" > outdir/subdir/file1
          echo "Contents of file 2 in subdir" > outdir/subdir/file2
          echo "Using directories are useful when you run a tool that results in outputs written to the directory"
        shell: bash

```

### `scans.[ID].steps[]`

Every job is composed of one or more steps to be taken in order to execute a job. It can be as simple as a single step running the script, or it can be a script completely written in a workflow. Each steps `stdout` and `stderr` is streamed back to the server and later displayed as a result.

If the step fails, the job fails. Failure of the step is based on the exit code of the script.

If step fails, the `stderr` is going to be displayed on the UI.

::alert{type="info"}
The step output endpoint is limited to 2MiB. However, the result endpoint is limited to 100MiB. So if the output of your tool is larger than 2MiB,
you should avoid writing it to the `stdout`.
::

### `scans.[ID].steps[].if`

Steps may conditionally be skipped. This is useful when you want to execute a step `if` something meets the condition. 
Let's try to illustrate this in a difficult example.

Our objectives for this example are:
- To run `httprobe` if either `assetfinder` is successful or `subfinder` is successful.
- Since there can be an instance where one of our dependency scans are done, while the other one is not, we need to conditionally download the result.
- If both of them are done, we should combine their latest outputs.

Keep in mind, in this example scan, we can have the following sequence of events:
1. `assetfinder` successfully finishes
2. `subfinder` is not scheduled yet so we schedule the `httprobe`.
3. `subfinder` is scheduled after httprobe.
4. `httprobe` is scheduled again, and we will re-use the latest data from the `assetfinder`. We don't care that we executed the `httprobe` on those subdomains that it found. Maybe, just maybe, some of those subdomains are up, and we want to test them.

```yaml
scans:
  assetfinder:
    uploads:
      - output.txt
    # ...
  subfinder:
    uploads:
      - output.txt
    # ...
  httprobe:
    on:
      expr: |
        (size(scans.assetfinder) > 0 && scans.assetfinder[0].status == 'succeeded') // If assetfinder has latest scan succeded
        || (size(scans.subfinder) > 0 && scans.subfinder[0].status == 'succeeded') // If subfinder has latest scan succeded
    steps:
      # Prepare the subdomains file
      - run: touch subdomains.txt
        shell: bash
      # Run if the latest assetfinder was successful
      - if: | 
          size(scans.assetfinder) > 0 
          && scans.assetfinder[0].status == 'succeeded'
        run: | 
          echo "Downloading assetfinder results using bh"
          echo "Makind directory to download the result to"
          mkdir assetfinder && cd assetfinder
          echo "Downloading the latest assetfinder result"
          bh job download -p ${{ project.id }} -w ${{ workflow.id }} -r ${{ revision.id }} -j ${{ scans.assetfinder[0].id }} -o output.zip
          echo "Unzipping the result"
          unzip output.zip
          rm output.zip
          echo "Writing the result into subdomains.txt"
          cd ..
          cat assetfinder/output.txt >> subdomains.txt
        shell: bash
      - if: |
          size(scans.subfinder) > 0 # Run if the latest subfinder was successful
          && scans.subfinder[0].status == 'succeeded'
        run: |
          echo "Downloading subfinder results using bh"
          echo "Makind directory to download the result to"
          mkdir subfinder && cd subfinder
          echo "Downloading the latest subfinder result"
          bh job download -p ${{ project.id }} -w ${{ workflow.id }} -r ${{ revision.id }} -j ${{ scans.subfinder[0].id }} -o output.zip
          echo "Unzipping the result"
          unzip output.zip
          rm output.zip
          echo "Writing the result into subdomains.txt"
          cd ..
          cat subfinder/output.txt >> subdomains.txt
        shell: bash
      - run: |
          echo "Ensuring results are unique and sorted"
          sort -o subdomains.txt -u subdomains.txt
        shell: bash
      - run: cat subdomains.txt | httprobe
        shell: bash
```

### `scans.[ID].steps[].run`

Run specifies a script to be executed. It is written to a temporary file on the disk and executed by a shell script specified in `scans.[ID].steps[].shell`.

The directory where the script is written depends on the way the runner is configured. Let's say that the runner is located at `/home/runner` and the `workdir` configured for the runner is `_work`, then the script will be temporarily written to the `/home/runner/_work/{job_id}/{uuid}`.

The script lives only during the step. It will be cleaned up after each step.

To write a script, you can use template evaluation syntax. To read more about that, please visit "Template evaluation" section.

::alert{type="warning"}
There may be situations where workflow syntax looks valid, but the server fails to parse it.

For example:
```yaml
- run: sleep $((sleep_time > 0 ? sleep_time : 0)); echo "test"
  shell: bash
```

To fix this problem, you can do the following:
```yaml
- run: |
    sleep $((sleep_time > 0 ? sleep_time : 0)); echo "test"
  shell: bash
```
::

### `scans.[ID].steps[].shell`

Shell specifies the executable that is going to execute the script. The runner uses shell as an argument to the `which` command. If the executable does not exist on the path, the step will fail. Right now, there is a limitation where you cannot provide arguments to the command. This may change in the future.

For example, let's say the step is the following:

```yaml
- run: echo "example"
  shell: bash
```

The steps the runner will take are:

1. Call `which bash`. Let's say the output is `/bin/bash`
1. Create a temporary file with a random name. For the random name, UUIDv4 is used. For simplicity, let's just refer to it as `{UUID}`
1. Take the content of the run and write it to a file.
1. Execute `/bin/bash {UUID}`
