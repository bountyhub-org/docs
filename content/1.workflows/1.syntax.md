# Workflows

Workflows are the way you describe automation. They should describe the way you do stuff in a concise and declarative way. Workflows should act as if there is a replica of you doing the stuff that can be automated. It should be a continuous process, feeding you information in the real time, allowing you to do the best work you possibly can.

Pipelines on the other hand are sequences of steps that are executed from start to finish. It is much easier to reason about the pipeline, but it is more natural to work in terms of workflows.

## Workflows vs Pipelines

The main difference between workflows and pipelines is that pipelines are executed from start to finish. You can have parallelism in pipelines, but essentially, you are passing through stages. Let's take a simple example to understand the difference.

#### Pipeline example:

![pipeline](/pipeline.svg)

The main problem here is that you either execute the entire pipeline, or you don't. But that is not the way you would approach a target. You would use common data to execute multiple pipelines, and this is where workflows kick in.

Let's say I start with subdomain discovery. You have multiple tools that run subdomain discovery to maximize the target scope. However, from there, you can have multiple pipelines. For example:

1. Screenshot all subdomain landing pages to quickly pick a target
2. Run httprobe to see what subdomains are live
3. Run port scan on each target.
4. ...

It is not that often that ports are open, and scan is noisy, so we can run it every other day or once a week. However, running httprobe is relatively quick, and does not raise alarms that often, so we can do it every 6 hours.

This is an example of how a single stage may have multiple pipelines. And you are already doing that right now, I would guess.

## Writing workflows

Workflows are yaml definitions where you describe what you want to do. YAML proved to be a great tool to describe your intent. It is a commonly used language to describe workflows.

Initially, writing workflows started with the UI editor, but proved to be much more complicated than the declarative way.

::alert{type="warning"}
Right now, the experience of writing workflows is horrible. Hopefully, language client will be implemented soon, but please, make sure to have this reference open if parsing fails...
::

The editor however does contain a basic syntax so you can leverage autocompletes to see what fields are available.

## Workflow components

### `scans[]`

Each workflow is made of one or more `scans[]`. Each scan has a unique name, as a required field on an object. Scans are listed as yaml lists.

You can have the unlimited number of scans, as long as their names are unique.

#### Example:

```yaml
scans:
  - name: assetfinder
    on:
      cron: 0 0 * * * *
    steps:
      - run: assetfinder example.com
        shell: bash
```

### `scans[].name`

Unique name assigned to each scan. This name can be used as a reference to a scan.

### `scans[].on`

Serves as a trigger for a job run. It can be based on a schedule, or on a different scan execution.

### `scans[].on.cron`

Cron defines a schedule based on which the workflows are executed. Each workflow must have at least one cron scan as a starting point of pipeline execution.

Cron is described in form of:

| second   | minute   | hour     | day of month | month    | day of week |
| -------- | -------- | -------- | ------------ | -------- | ----------- |
| required | required | required | required     | required | required    |

Example:

```yaml
scans:
  - name: assetfinder
    on:
      cron: 0 * * * * *
```

### `scans[].on.dependency[]`

Dependency declares the event trigger based on the `trigger` field. To be runnable, a dependency chain must begin with a scan that is executed on schedule.

So if scan A specifies dependency on scan B, then scan B must either be triggered on `cron`, or have a dependency somewhere in the chain that is triggered on `cron`.

A scan can have multiple triggers, and therefore the dependency field is a list. If any trigger evaluates to true, the scan is scheduled.

There may be situations where scan A depends on two scans, let's call them scan B and scan C. Once scan B finishes and meets the scheduling requirement of scan A, the scan A will be scheduled. However, during runner request, if all dependencies are not met at the time of assignment, the scan will be cancelled and subject to later scheduling.

### `scans[].on.dependency[].name`

Scan defines a references the scan by `scans[].name` that it depends on.

### `scans[].on.dependency[].trigger`

Trigger defines a specific event that, when evaluated to true, schedules the dependent scan.

Trigger can be:

- "diff": If the content of the dependency scan changes in any way, the dependent scan is scheduled.
- "done": Every time dependency scan is executed **successfully**, dependent scan is scheduled.

If dependent scan `A` has a step that uses the output of the dependency scan `B` (e.g. `${{ dependency.B }}`), the latest successful execution of scan `B` will be downloaded by the runner and thus can be used in a workflow.

### `scans[].steps[]`

Every job is composed of one or more steps to be taken in order to execute a job. It can be as simple as a single step running the script, or it can be a script completely written in a workflow. Each steps `stdout` and `stderr` is streamed back to the server and later displayed as a result.

If the step fails, the job fails. Failure of the step is based on the exit code of the script.

If step fails, the `stderr` is going to be displayed on the UI.

::alert{type="info"}
The step output endpoint is limited to 2MiB. However, the result endpoint is limited to 100MiB. So if the output of your tool is larger than 2MiB,
you should avoid writing it to the `stdout`.
::

### `scans[].steps[].run`

Run specifies a script to be executed. It is written to a temporary file on the disk and executed by a shell script specified in `scans[].steps[].shell`.

The directory where the script is written depends on the way the runner is configured. Let's say that the runner is located at `/home/runner` and the `workdir` configured for the runner is `_work`, then the script will be temporarily written to the `/home/runner/_work/{job_id}/{uuid}`.

The script lives only during the step. It will be cleaned up after each step.

To write a script, you can use template evaluation syntax. To read more about that, please visit "Template evaluation" section.

There may be situations where workflow syntax looks valid, but the server fails to parse it.

For example:
```yaml
- run: sleep $((sleep_time > 0 ? sleep_time : 0)); echo "test"
  shell: bash
```

To fix this problem, you can do the following:
```yaml
- run: |
    sleep $((sleep_time > 0 ? sleep_time : 0)); echo "test"
  shell: bash
```

### `scans[].steps[].shell`

Shell specifies the executable that is going to execute the script. The runner uses shell as an argument to the `which` command. If the executable does not exist on the path, the step will fail.

For example, let's say the step is the following:

```yaml
- run: echo "example"
  shell: bash
```

The steps the runner will take are:

1. Call `which bash`. Let's say the output is `/bin/bash`
1. Create a temporary file with a random name. For the random name, UUIDv4 is used. For simplicity, let's just refer to it as `{UUID}`
1. Take the content of the run and write it to a file.
1. Execute `/bin/bash {UUID}`

### `groups[]`

Groups are a way to visualize and compose multiple scans into a single view. It may be useful for example in subdomain discoveries to combine them in a single output.

### `groups[].name`

Unique name that describes a group. It may also be used in a workflow template as an input to a scan. However, it may not be used as a dependency.

### `groups[].uses[]`

A list of scan references related to scans used in a group.

### `groups[].uses[].scan`

A `string` type referencing a scan name. It is used as a ref. Once the scan is successful, the output will be used in a output. Otherwise, it won't be used.

### `groups[].uses[].file`

A file reference that is an output file. If the scan does not have the output, the output won't be used. Since the outputs can't be checked, please use a correct reference to a file.

### `groups[].uses[].grep`

Grep is a regexp. This may be handy if your scans are outputing more data. Let's say subdomain discovery produces sources and ip addresses. But you are only interested in ip addresses. Then you can leverage grep feature to extract ip addresses so the output will only show that.

Grep is line based. If the field is empty, the entire line will be used. Grep allows you to specify index based references, and named references.

Grep library used behind the scenes is located [here](https://docs.rs/regex/latest/regex/#grouping-and-flags), so you can leverage captures that are supported by the library.

#### Index based references

```yaml
groups:
  - name: subdomaindiscovery
    uses:
      - scan: assetfinder
        file: assetfinder.out
        grep: (.*) # Define a pattern. Here, use the entire line
        extract: $1
      - scan: subfinder
        file: subfinder.out # Since no pattern is used, defaults to the entire line

scans:
  - name: assetfinder
    on:
      cron: 0 * * * * *
    steps:
      - run: assetfinder ${{ var.DOMAIN }} | tee $BH_RESULT/assetfinder.out
        shell: bash
  - name: subfinder
    on:
      cron: 30 * * * * *
    steps:
      - run: subfinder ${{ var.DOMAIN }} | tee $BH_RESULT/subfinder.out
        shell: bash
  # ...
```

#### Name based references

```yaml
groups:
  - name: subdomaindiscovery
    uses:
      - scan: assetfinder
        file: assetfinder.out
        grep: (?<domain>.*)
        extract: $domain
```

### `groups[].uses[].extract`

Extract allows you to extract parts of your regexp. You can use index based extraction like `$1` for example, or you can use named extraction like `$ip`

See library used for grep [here](https://docs.rs/regex/latest/regex/#grouping-and-flags).
